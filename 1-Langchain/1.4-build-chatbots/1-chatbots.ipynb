{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() ## load env variable\n",
    "\n",
    "groq_api_key = os.getenv(\"GROG_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Hai! üëã\\n\\nIt's nice to meet you. What can I do for you today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 15, 'total_tokens': 42, 'completion_time': 0.049090909, 'prompt_time': 8.14e-05, 'queue_time': 0.015617250000000001, 'total_time': 0.049172309}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-39023a3d-76c3-4b83-9400-975ebda035ec-0', usage_metadata={'input_tokens': 15, 'output_tokens': 27, 'total_tokens': 42})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Hai\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Hai and that you are a Software Engineer!  \\n\\nIs there anything else you'd like to talk about? üòÑ  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 58, 'total_tokens': 93, 'completion_time': 0.063636364, 'prompt_time': 0.001842193, 'queue_time': 0.011584984999999999, 'total_time': 0.065478557}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-478165c8-752c-4e25-9328-325ade947790-0', usage_metadata={'input_tokens': 58, 'output_tokens': 35, 'total_tokens': 93})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Hai and I am a Software Engineer\"),\n",
    "        AIMessage(content=\"Hello Hai! It's nice to meet you. What can I do for you today\"),\n",
    "        HumanMessage(content=\"Hey Whats my name and What I do to\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, My name is Hai and I am a Software Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Hai, it's nice to meet you!\\n\\nAs a Software Engineer, what kind of projects are you working on these days?  \\n\\nI'm always interested in learning more about the work people do in the tech industry.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Hai.  \\n\\nI remember that you introduced yourself at the beginning of our conversation. üòä  \\n\\n\\nLet me know if you have any other questions!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 84, 'total_tokens': 120, 'completion_time': 0.065454545, 'prompt_time': 0.003233217, 'queue_time': 0.011161143, 'total_time': 0.068687762}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-704ca305-d18d-49de-bb2f-dbf0571af556-0', usage_metadata={'input_tokens': 84, 'output_tokens': 36, 'total_tokens': 120})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What my name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to use it! üòä\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Change the config --> session id\n",
    "config1 = {\"configurable\" : {\"session_id\": \"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What my name?\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Hai and I am a Software Engineer', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Hai, it's nice to meet you!\\n\\nAs a Software Engineer, what kind of projects are you working on these days?  \\n\\nI'm always interested in learning more about the work people do in the tech industry.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 21, 'total_tokens': 72, 'completion_time': 0.092727273, 'prompt_time': 0.000146779, 'queue_time': 0.014175791, 'total_time': 0.092874052}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2cb048a9-2a18-445f-b0e7-1d44119e7402-0', usage_metadata={'input_tokens': 21, 'output_tokens': 51, 'total_tokens': 72}), HumanMessage(content='What my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Hai.  \\n\\nI remember that you introduced yourself at the beginning of our conversation. üòä  \\n\\n\\nLet me know if you have any other questions!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 84, 'total_tokens': 120, 'completion_time': 0.065454545, 'prompt_time': 0.003233217, 'queue_time': 0.011161143, 'total_time': 0.068687762}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-704ca305-d18d-49de-bb2f-dbf0571af556-0', usage_metadata={'input_tokens': 84, 'output_tokens': 36, 'total_tokens': 120})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='What my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to use it! üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 13, 'total_tokens': 53, 'completion_time': 0.072727273, 'prompt_time': 8.5e-05, 'queue_time': 0.01429829, 'total_time': 0.072812273}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-490d3e54-708b-4940-81bb-7c754d1da5ee-0', usage_metadata={'input_tokens': 13, 'output_tokens': 40, 'total_tokens': 53})])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Hai, it's nice to meet you! üëã \\n\\nHow can I help you today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 19, 'total_tokens': 46, 'completion_time': 0.049090909, 'prompt_time': 8.193e-05, 'queue_time': 0.013714450000000001, 'total_time': 0.049172839}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-584f5200-2144-4544-a725-ae2594741795-0', usage_metadata={'input_tokens': 19, 'output_tokens': 27, 'total_tokens': 46})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Hai\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Hai, it's nice to meet you! üëã  What can I do for you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 14, 'total_tokens': 38, 'completion_time': 0.043636364, 'prompt_time': 9.3339e-05, 'queue_time': 0.014649529, 'total_time': 0.043729703}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-c3fd452a-d0f7-4c23-ab56-c24931d0280b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 24, 'total_tokens': 38})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\" : {\"session_id\": \"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Hai\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are helpful assistant. Answer all question to the best of your ability in {language}.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xin ch√†o Hai! R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n. \\n\\nB·∫°n c√≥ mu·ªën h·ªèi t√¥i ƒëi·ªÅu g√¨ kh√¥ng? T√¥i s·∫Ω c·ªë g·∫Øng h·∫øt s·ª©c ƒë·ªÉ tr·∫£ l·ªùi b·∫°n m·ªôt c√°ch ƒë·∫ßy ƒë·ªß v√† ch√≠nh x√°c nh·∫•t. \\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Hi my name is Hai\")], \"language\": \"Vietnamese\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xin ch√†o H·∫£i! \\n\\nR·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n. üòä  \\n\\nB·∫°n mu·ªën h·ªèi t√¥i ƒëi·ªÅu g√¨ kh√¥ng? \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\" : {\"session_id\": \"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I am Hai\")], \"language\": \"Vietnamese\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, can you help me with AI?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Of course! What would you like help with?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I need help with understanding about GenAI.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hello, can you help me with AI?\"),\n",
    "    AIMessage(content=\"Of course! What would you like help with?\"),\n",
    "    HumanMessage(content=\"I need help with understanding about GenAI.\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"B·∫°n mu·ªën t√¨m hi·ªÉu v·ªÅ GenAI. \\n\\n(You asked about understanding GenAI.) \\n\\n\\nIs there anything specific about GenAI that you'd like to know? For example, are you curious about:\\n\\n* **What GenAI is?**\\n* **How it works?**\\n* **Its applications?**\\n* **The ethical implications?**\\n\\n\\n\\nI'm here to help you understand! Just let me know what you're most interested in. üòä\\n\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is question I asked\")],\n",
    "        \"language\": \"Vietnamese\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrap in Message History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "config = {\"configurable\" : {\"session_id\": \"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B·∫°n c·∫ßn gi√∫p ƒë·ª° v·ªÅ vi·ªác hi·ªÉu GenAI. \\n\\n\\nYou asked: \"I need help with understanding about GenAI.\"\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is question I asked\")],\n",
    "        \"language\": \"Vietnamese\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
